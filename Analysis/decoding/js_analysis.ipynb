{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "import sys, os\n",
    "root = '/usr/local/serenceslab/maggie/shapeDim/'\n",
    "\n",
    "sys.path.append(os.path.join(root, 'Analysis', 'code_utils'))\n",
    "import data_utils\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data and do x-session decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading S01, main task\n",
      "loading S01, repeat task\n",
      "ROI: V1, Session: 1\n",
      "ROI: V1, Session: 2\n",
      "ROI: V1, Session: 3\n",
      "ROI: V2, Session: 1\n",
      "ROI: V2, Session: 2\n",
      "ROI: V2, Session: 3\n",
      "ROI: V3, Session: 1\n",
      "ROI: V3, Session: 2\n",
      "ROI: V3, Session: 3\n",
      "ROI: V3AB, Session: 1\n",
      "ROI: V3AB, Session: 2\n",
      "ROI: V3AB, Session: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/AD/mmhender/anaconda3/envs/shapedim/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROI: hV4, Session: 1\n",
      "ROI: hV4, Session: 2\n",
      "ROI: hV4, Session: 3\n",
      "ROI: IPS0, Session: 1\n",
      "ROI: IPS0, Session: 2\n",
      "ROI: IPS0, Session: 3\n",
      "ROI: IPS1, Session: 1\n",
      "ROI: IPS1, Session: 2\n",
      "ROI: IPS1, Session: 3\n",
      "ROI: IPS2, Session: 1\n",
      "ROI: IPS2, Session: 2\n",
      "ROI: IPS2, Session: 3\n",
      "ROI: IPS3, Session: 1\n",
      "ROI: IPS3, Session: 2\n",
      "ROI: IPS3, Session: 3\n",
      "ROI: LO1, Session: 1\n",
      "ROI: LO1, Session: 2\n",
      "ROI: LO1, Session: 3\n",
      "ROI: LO2, Session: 1\n",
      "ROI: LO2, Session: 2\n",
      "ROI: LO2, Session: 3\n"
     ]
    }
   ],
   "source": [
    "subjects = [1]#,2,3,4,5,7]\n",
    "\n",
    "n_subj = len(subjects)\n",
    "\n",
    "n_rois = 11\n",
    "\n",
    "n_sess = 3\n",
    "\n",
    "# penalties to eval\n",
    "num_cs = 20\n",
    "Cs = np.logspace( -5,0,num_cs )\n",
    "\n",
    "# define model\n",
    "model = LogisticRegressionCV( Cs=Cs, cv = 5, multi_class='multinomial', solver='lbfgs', penalty='l2', n_jobs = 8 )\n",
    "\n",
    "# store acc...\n",
    "acc = np.full( ( n_subj, n_rois, n_sess, n_sess, 6 ), np.nan )\n",
    "\n",
    "\n",
    "task_names = ['Linear (1)','Linear (2)','Checker'];\n",
    "n_tasks = len(task_names)\n",
    "\n",
    "# three different ways to do binary decoding\n",
    "n_bounds = 3;\n",
    "bound_names = ['Decode: Linear (1)','Decode: Linear (2)','Decode: Checker'];\n",
    "quad_groups = [[[1, 4], [2, 3]],\n",
    "                [[1, 2], [3, 4]],\n",
    "                [[1, 3], [2, 4]]];\n",
    "\n",
    "make_time_resolved = False\n",
    "\n",
    "\n",
    "\n",
    "# first load all data for all subjects, both tasks\n",
    "maindat_all = []; repdat_all = []\n",
    "mainlabs_all = []; replabs_all = []\n",
    "\n",
    "for si, ss in enumerate(subjects):\n",
    "    # si = 0; ss = 1;\n",
    "\n",
    "    print('loading S%02d, main task'%ss)\n",
    "    main_data, _, main_labels, roi_names = data_utils.load_main_task_data(ss, make_time_resolved)\n",
    "\n",
    "    for ri in range(n_rois):\n",
    "        \n",
    "        # subtract mean across voxels each trial\n",
    "        main_data[ri] -= np.tile(np.mean(main_data[ri], axis=1, keepdims=True), [1, main_data[ri].shape[1]])\n",
    "\n",
    "    maindat_all += [main_data]\n",
    "    mainlabs_all += [main_labels]\n",
    "\n",
    "    print('loading S%02d, repeat task'%ss)\n",
    "    rep_data, _, rep_labels, roi_names = data_utils.load_repeat_task_data(ss, make_time_resolved)\n",
    "\n",
    "    for ri in range(n_rois):\n",
    "        # subtract mean across voxels each trial\n",
    "        rep_data[ri] -= np.tile(np.mean(rep_data[ri], axis=1, keepdims=True), [1, rep_data[ri].shape[1]])\n",
    "\n",
    "    repdat_all += [rep_data]\n",
    "    replabs_all += [rep_labels]\n",
    "    \n",
    "    ## make a new df that only has main grid trials\n",
    "    grid_df = main_labels[ main_labels[ 'is_main_grid' ] == 1 ]\n",
    "\n",
    "    # filter the data from each roi to only include main grid trials\n",
    "    # move from list to dict so that roi name is the key (just my preference)\n",
    "    grid_data = {}\n",
    "\n",
    "    for r_idx, r in enumerate( roi_names ):\n",
    "\n",
    "        grid_data[ r ] = main_data[ r_idx ][ main_labels[ 'is_main_grid' ] == 1 ] \n",
    "\n",
    "\n",
    "    # how many sessions\n",
    "    n_sess = np.max( np.max( grid_df[ 'sess' ] ) )\n",
    "\n",
    "    # how many parts in each session - check each separately...\n",
    "    tmp = []\n",
    "    for s in range( 1,n_sess+1 ):\n",
    "        tmp.append( np.max( grid_df[ grid_df['sess'] == s ][ 'part' ] ) )\n",
    "    \n",
    "    n_parts = np.min( tmp ) \n",
    "    \n",
    "    ## figure out grid position on each trial\n",
    "    # get all x,y values\n",
    "    x = np.array( grid_df[ 'ptx' ] )\n",
    "    y = np.array( grid_df[ 'pty' ] )\n",
    "\n",
    "    # meshgrid for all combos\n",
    "    X,Y = np.meshgrid( np.unique(x),np.unique(y) )\n",
    "\n",
    "    # figure out all unique x,y combos and put into array\n",
    "    pnts = np.vstack( ( X.ravel(),Y.ravel() ) ).T\n",
    "\n",
    "    # to store grid pos of each stim\n",
    "    grid_pos = np.full( len( x ),np.nan )\n",
    "\n",
    "    # loop over all trials\n",
    "    for i in range( len( x ) ):\n",
    "\n",
    "        # loop over unique points\n",
    "        for j in range( len( pnts ) ):\n",
    "\n",
    "            # find the match for current x,y pair\n",
    "            if np.sum( [x[i],y[i]]==pnts[j,] ) == 2:\n",
    "\n",
    "                # store\n",
    "                grid_pos[i] = j\n",
    "\n",
    "    # double check num unique vals + overall length\n",
    "    assert( len( np.unique( grid_pos ) ) == 16 )\n",
    "    assert( len( grid_pos ) == grid_data['V1'].shape[0] )    \n",
    "    \n",
    "    ## Model/predict...just do v1 at first...\n",
    "    # loop over ROIs\n",
    "    for r_idx, r in enumerate( roi_names ):\n",
    "\n",
    "        # get the data from this ROI\n",
    "        X = grid_data[ r ]\n",
    "\n",
    "        # loop over sessions...train/test on all combos (e.g. sess1-1, sess1-2, sess1-3, etc)    \n",
    "        for s in np.arange( 1,n_sess+1 ):\n",
    "\n",
    "            # progress...\n",
    "            print(f'ROI: {r}, Session: {s}')\n",
    "\n",
    "            # loop over parts\n",
    "            for p in np.arange( 1,n_parts+1 ):\n",
    "\n",
    "                # training data from this session, all parts except current part...\n",
    "                train_X = X[ ( grid_df['sess'] == s ) & ( grid_df['part'] != p ) ]\n",
    "                train_y = grid_pos[ ( grid_df['sess'] == s ) & ( grid_df['part'] != p ) ]\n",
    "\n",
    "                # train model...\n",
    "                model.fit( train_X, train_y )\n",
    "\n",
    "                # test data same session\n",
    "                test_X = X[ ( grid_df['sess'] == s ) & ( grid_df['part'] == p ) ]\n",
    "                test_y = grid_pos[ ( grid_df['sess'] == s ) & ( grid_df['part'] == p ) ]\n",
    "\n",
    "                # predict\n",
    "                acc[ si, r_idx, s-1, s-1, p-1 ] = model.score( test_X,test_y )\n",
    "\n",
    "                # cross-gen from current session to other sessions...\n",
    "                for cg_s in np.setdiff1d( range( 1,n_sess+1 ), s ):\n",
    "\n",
    "                    # test data same session\n",
    "                    test_X = X[ ( grid_df['sess'] == cg_s ) & ( grid_df['part'] == p ) ]\n",
    "                    test_y = grid_pos[ ( grid_df['sess'] == cg_s ) & ( grid_df['part'] == p ) ]      \n",
    "\n",
    "                    # predict\n",
    "                    acc[ si, r_idx, s-1, cg_s-1, p-1 ] = model.score( test_X,test_y )\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.16898148, 0.20572917, 0.17534722, 0.13136574, 0.0865162 ,\n",
       "       0.07581019, 0.08015046, 0.06741898, 0.08043981, 0.11082176,\n",
       "       0.08159722])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.mean(np.mean(acc[0,:,:,:,:], axis=1), axis=1), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify object as a function of task context...\n",
    "* so train on task 1, generalize to task 2,3 in same session and in other sessions, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = [1,2,3,4,5,7]\n",
    "\n",
    "n_subj = len(subjects)\n",
    "\n",
    "n_rois = 11\n",
    "\n",
    "n_sess = 3\n",
    "\n",
    "# task names...\n",
    "task_names = ['Linear (1)','Linear (2)','Checker'];\n",
    "n_tasks = len(task_names)\n",
    "\n",
    "# three different ways to do binary decoding\n",
    "n_bounds = 3;\n",
    "bound_names = ['Decode: Linear (1)','Decode: Linear (2)','Decode: Checker'];\n",
    "quad_groups = [[[1, 4], [2, 3]],\n",
    "                [[1, 2], [3, 4]],\n",
    "                [[1, 3], [2, 4]]];\n",
    "\n",
    "# penalties to eval\n",
    "num_cs = 20\n",
    "Cs = np.logspace( -5,0,num_cs )\n",
    "\n",
    "# define model\n",
    "model = LogisticRegressionCV( Cs=Cs, cv = 5, multi_class='multinomial', solver='lbfgs', penalty='l2', n_jobs = 8 )\n",
    "\n",
    "# store acc...\n",
    "acc = np.full( ( n_subj, n_rois, n_tasks, n_tasks, n_sess ), np.nan )\n",
    "\n",
    "make_time_resolved = False\n",
    "\n",
    "\n",
    "\n",
    "# first load all data for all subjects, both tasks\n",
    "maindat_all = []; repdat_all = []\n",
    "mainlabs_all = []; replabs_all = []\n",
    "\n",
    "for si, ss in enumerate(subjects):\n",
    "    # si = 0; ss = 1;\n",
    "\n",
    "    print('loading S%02d, main task'%ss)\n",
    "    main_data, _, main_labels, roi_names = data_utils.load_main_task_data(ss, make_time_resolved)\n",
    "\n",
    "    for ri in range(n_rois):\n",
    "        \n",
    "        # subtract mean across voxels each trial\n",
    "        main_data[ri] -= np.tile(np.mean(main_data[ri], axis=1, keepdims=True), [1, main_data[ri].shape[1]])\n",
    "\n",
    "    maindat_all += [main_data]\n",
    "    mainlabs_all += [main_labels]\n",
    "\n",
    "    print('loading S%02d, repeat task'%ss)\n",
    "    rep_data, _, rep_labels, roi_names = data_utils.load_repeat_task_data(ss, make_time_resolved)\n",
    "\n",
    "    for ri in range(n_rois):\n",
    "        # subtract mean across voxels each trial\n",
    "        rep_data[ri] -= np.tile(np.mean(rep_data[ri], axis=1, keepdims=True), [1, rep_data[ri].shape[1]])\n",
    "\n",
    "    repdat_all += [rep_data]\n",
    "    replabs_all += [rep_labels]\n",
    "    \n",
    "    ## make a new df that only has main grid trials\n",
    "    grid_df = main_labels[ main_labels[ 'is_main_grid' ] == 1 ]\n",
    "\n",
    "    # filter the data from each roi to only include main grid trials\n",
    "    # move from list to dict so that roi name is the key (just my preference)\n",
    "    grid_data = {}\n",
    "\n",
    "    for r_idx, r in enumerate( roi_names ):\n",
    "\n",
    "        grid_data[ r ] = main_data[ r_idx ][ main_labels[ 'is_main_grid' ] == 1 ] \n",
    "\n",
    "\n",
    "    # how many sessions\n",
    "    n_sess = np.max( np.max( grid_df[ 'sess' ] ) )\n",
    "\n",
    "    # how many parts in each session - check each separately...\n",
    "    tmp = []\n",
    "    for s in range( 1,n_sess+1 ):\n",
    "        tmp.append( np.max( grid_df[ grid_df['sess'] == s ][ 'part' ] ) )\n",
    "    \n",
    "    n_parts = np.min( tmp ) \n",
    "    \n",
    "    ## figure out grid position on each trial\n",
    "    # get all x,y values\n",
    "    x = np.array( grid_df[ 'ptx' ] )\n",
    "    y = np.array( grid_df[ 'pty' ] )\n",
    "\n",
    "    # meshgrid for all combos\n",
    "    X,Y = np.meshgrid( np.unique(x),np.unique(y) )\n",
    "\n",
    "    # figure out all unique x,y combos and put into array\n",
    "    pnts = np.vstack( ( X.ravel(),Y.ravel() ) ).T\n",
    "\n",
    "    # to store grid pos of each stim\n",
    "    grid_pos = np.full( len( x ),np.nan )\n",
    "\n",
    "    # loop over all trials\n",
    "    for i in range( len( x ) ):\n",
    "\n",
    "        # loop over unique points\n",
    "        for j in range( len( pnts ) ):\n",
    "\n",
    "            # find the match for current x,y pair\n",
    "            if np.sum( [x[i],y[i]]==pnts[j,] ) == 2:\n",
    "\n",
    "                # store\n",
    "                grid_pos[i] = j\n",
    "\n",
    "    # double check num unique vals + overall length\n",
    "    assert( len( np.unique( grid_pos ) ) == 16 )\n",
    "    assert( len( grid_pos ) == grid_data['V1'].shape[0] )    \n",
    "    \n",
    "    \n",
    "    ## Model/predict...loop over ROIs\n",
    "    for r_idx, r in enumerate( roi_names ):\n",
    "\n",
    "        # get the data from this ROI\n",
    "        X = grid_data[ r ]\n",
    "\n",
    "        # loop over sessions...train/test on all combos (e.g. sess1-1, sess1-2, sess1-3, etc)    \n",
    "        for s_idx,s in enumerate( np.arange( 1,n_sess+1 ) ):\n",
    "\n",
    "            # progress...\n",
    "            print(f'ROI: {r}, Session: {s}')\n",
    "\n",
    "            # loop over task...\n",
    "            for t_idx,t in enumerate( np.arange( 1,4 ) ):\n",
    "\n",
    "                # training data from this session, task 1\n",
    "                train_X = X[ ( grid_df['sess'] == s ) & ( grid_df['task'] == t ) ]\n",
    "                train_y = grid_pos[ ( grid_df['sess'] == s ) & ( grid_df['task'] == t ) ]\n",
    "\n",
    "                # train model...\n",
    "                model.fit( train_X, train_y )\n",
    "\n",
    "                # then generalize to other tasks...first same session, other tasks, then diff\n",
    "                # sessions, other tasks. \n",
    "                for cgt_idx,cgt in enumerate( np.setdiff1d( range( 1,4 ), t ) ):\n",
    "\n",
    "                    # test data same session\n",
    "                    test_X = X[ ( grid_df['sess'] == s ) & ( grid_df['task'] == cgt ) ]\n",
    "                    test_y = grid_pos[ ( grid_df['sess'] == s ) & ( grid_df['task'] == cgt ) ] \n",
    "\n",
    "                    acc[ si, r_idx, t_idx, cgt-1, s_idx ] = model.score( test_X,test_y )\n",
    "\n",
    "#                     # cross-gen from current session to other sessions...\n",
    "#                     for cgs_idx,cgs in enumerate( np.setdiff1d( range( 1,n_sess+1 ), s ) ):\n",
    "\n",
    "#                         # test data same session\n",
    "#                         test_X = X[ ( grid_df['sess'] == cgs ) & ( grid_df['task'] == t1[ trn_idx ] ) ]\n",
    "#                         test_y = grid_pos[ ( grid_df['sess'] == cgs ) & ( grid_df['task'] == t1[ trn_idx ] ) ]      \n",
    "\n",
    "#                         # predict\n",
    "#                         acc[ si, r_idx, t_idx, cgt-1, s_idx, cgs-1 ] = model.score( test_X,test_y )\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na = np.nanmean(acc, axis = 0)\n",
    "\n",
    "na[0,].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.sin(np.linspace(0,2*np.pi,1000)) * np.random.random(1000)*100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na = np.nanmean( na, axis = 2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.pi\n",
    "y = 2/3 * np.pi\n",
    "np.arctan2(y,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi = 7\n",
    "print(roi_names[roi])\n",
    "\n",
    "na = np.nanmean(acc, axis = 0)\n",
    "na = np.nanmean( na, axis = 0)\n",
    "#na = np.nanmean( na[roi,], axis = 0 )\n",
    "na = np.nanmean( na, axis = 0 )\n",
    "na = np.nanmean( na, axis = 0 )\n",
    "\n",
    "plt.plot(na)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'na' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-17c0e0be864e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mroi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'na' is not defined"
     ]
    }
   ],
   "source": [
    "roi = 0\n",
    "data = np.nanmean(na[roi,:2,:2,],axis = 0)\n",
    "data = np.nanmean(data,axis = 0)\n",
    "\n",
    "cgs = [[0,1],[0,2],[1,0],[1,2],[2,0],[2,1]]\n",
    "\n",
    "# loop over cross-gen...\n",
    "#for cg in cgs:\n",
    "plt.imshow( data )\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(data[:,0])\n",
    "plt.plot(data[:,1])\n",
    "plt.plot(data[:,2])\n",
    "plt.legend(['Trn1','Trn2','Trn3'])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.diag(data))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diag( data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(na)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(na[0,:])\n",
    "plt.plot(na[1,:])\n",
    "plt.plot(na[2,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r_idx,r in enumerate( roi_names ):\n",
    "  \n",
    "    md = np.nanmean(acc[:,r_idx,:,:,:],axis=0)\n",
    "    md = np.nanmean(md,axis=2)\n",
    "\n",
    "    plt.imshow(md, cmap = 'RdBu',vmin = 0, vmax = .2)\n",
    "    \n",
    "    plt.title(r)\n",
    "    \n",
    "    plt.colorbar()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data and do x-sess decoding of task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df['task'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = [1,2,3,4,5,7]\n",
    "\n",
    "n_subj = len(subjects)\n",
    "\n",
    "n_rois = 11\n",
    "\n",
    "task_names = ['Linear (1)','Linear (2)','Checker'];\n",
    "n_tasks = len(task_names)\n",
    "\n",
    "# penalties to eval\n",
    "num_cs = 20\n",
    "Cs = np.logspace( -5,0,num_cs )\n",
    "\n",
    "# define model\n",
    "model = LogisticRegressionCV( Cs=Cs, cv = n_parts-1, multi_class='multinomial', solver='lbfgs', penalty='l2', n_jobs = 8 )\n",
    "\n",
    "# store acc...\n",
    "acc = np.full( ( n_subj, n_rois, n_sess, n_sess, 6 ), np.nan )\n",
    "\n",
    "make_time_resolved = False\n",
    "\n",
    "# first load all data for all subjects, both tasks\n",
    "maindat_all = []; repdat_all = []\n",
    "mainlabs_all = []; replabs_all = []\n",
    "\n",
    "for si, ss in enumerate(subjects):\n",
    "    # si = 0; ss = 1;\n",
    "\n",
    "    print('loading S%02d, main task'%ss)\n",
    "    main_data, _, main_labels, roi_names = data_utils.load_main_task_data(ss, make_time_resolved)\n",
    "\n",
    "    for ri in range(n_rois):\n",
    "        \n",
    "        # subtract mean across voxels each trial\n",
    "        main_data[ri] -= np.tile(np.mean(main_data[ri], axis=1, keepdims=True), [1, main_data[ri].shape[1]])\n",
    "\n",
    "    maindat_all += [main_data]\n",
    "    mainlabs_all += [main_labels]\n",
    "\n",
    "    print('loading S%02d, repeat task'%ss)\n",
    "    rep_data, _, rep_labels, roi_names = data_utils.load_repeat_task_data(ss, make_time_resolved)\n",
    "\n",
    "    for ri in range(n_rois):\n",
    "        # subtract mean across voxels each trial\n",
    "        rep_data[ri] -= np.tile(np.mean(rep_data[ri], axis=1, keepdims=True), [1, rep_data[ri].shape[1]])\n",
    "\n",
    "    repdat_all += [rep_data]\n",
    "    replabs_all += [rep_labels]\n",
    "    \n",
    "    ## make a new df that only has main grid trials\n",
    "    grid_df = main_labels[ main_labels[ 'is_main_grid' ] == 1 ]\n",
    "\n",
    "    # filter the data from each roi to only include main grid trials\n",
    "    # move from list to dict so that roi name is the key (just my preference)\n",
    "    grid_data = {}\n",
    "\n",
    "    for r_idx, r in enumerate( roi_names ):\n",
    "\n",
    "        grid_data[ r ] = main_data[ r_idx ][ main_labels[ 'is_main_grid' ] == 1 ] \n",
    "\n",
    "\n",
    "    # how many sessions\n",
    "    n_sess = np.max( np.max( grid_df[ 'sess' ] ) )\n",
    "\n",
    "    # how many parts in each session - check each separately...\n",
    "    tmp = []\n",
    "    for s in range( 1,n_sess+1 ):\n",
    "        tmp.append( np.max( grid_df[ grid_df['sess'] == s ][ 'part' ] ) )\n",
    "    \n",
    "    n_parts = np.min( tmp ) \n",
    "    \n",
    "    ## figure out grid position on each trial\n",
    "    # get all x,y values\n",
    "    x = np.array( grid_df[ 'ptx' ] )\n",
    "    y = np.array( grid_df[ 'pty' ] )\n",
    "\n",
    "    # meshgrid for all combos\n",
    "    X,Y = np.meshgrid( np.unique(x),np.unique(y) )\n",
    "\n",
    "    # figure out all unique x,y combos and put into array\n",
    "    pnts = np.vstack( ( X.ravel(),Y.ravel() ) ).T\n",
    "\n",
    "    # to store grid pos of each stim\n",
    "    grid_pos = np.full( len( x ),np.nan )\n",
    "\n",
    "    # loop over all trials\n",
    "    for i in range( len( x ) ):\n",
    "\n",
    "        # loop over unique points\n",
    "        for j in range( len( pnts ) ):\n",
    "\n",
    "            # find the match for current x,y pair\n",
    "            if np.sum( [x[i],y[i]]==pnts[j,] ) == 2:\n",
    "\n",
    "                # store\n",
    "                grid_pos[i] = j\n",
    "\n",
    "    # double check num unique vals + overall length\n",
    "    assert( len( np.unique( grid_pos ) ) == 16 )\n",
    "    assert( len( grid_pos ) == grid_data['V1'].shape[0] )    \n",
    "    \n",
    "    ## Model/predict...just do v1 at first...so slow :(\n",
    "    # loop over ROIs\n",
    "    for r_idx, r in enumerate( roi_names ):\n",
    "\n",
    "        # get the data from this ROI\n",
    "        X = grid_data[ r ]\n",
    "        y = grid_df['task']\n",
    "        \n",
    "        # loop over sessions...train/test on all combos (e.g. sess1-1, sess1-2, sess1-3, etc)    \n",
    "        for s in np.arange( 1,n_sess+1 ):\n",
    "\n",
    "            # progress...\n",
    "            print(f'ROI: {r}, Session: {s}')\n",
    "\n",
    "            # loop over parts\n",
    "            for p in range(1,3):\n",
    "\n",
    "                if p==1:\n",
    "\n",
    "                    # training data from this session, all parts except current part...\n",
    "                    train_X = X[ ( grid_df['sess'] == s ) & ( grid_df['part'] < 4 ) ]\n",
    "                    train_y = y[ ( grid_df['sess'] == s ) & ( grid_df['part'] < 4 ) ]\n",
    "\n",
    "                    # train model...\n",
    "                    model.fit( train_X, train_y )\n",
    "\n",
    "                    # test data same session\n",
    "                    test_X = X[ ( grid_df['sess'] == s ) & ( grid_df['part'] > 3 ) ]\n",
    "                    test_y = y[ ( grid_df['sess'] == s ) & ( grid_df['part'] > 3 ) ]\n",
    "\n",
    "                    # predict\n",
    "                    acc[ si, r_idx, s-1, s-1, p-1 ] = model.score( test_X,test_y )\n",
    "\n",
    "                    # cross-gen from current session to other sessions...\n",
    "                    for cg_s in np.setdiff1d( range( 1,n_sess+1 ), s ):\n",
    "\n",
    "                        # test data same session\n",
    "                        test_X = X[ ( grid_df['sess'] == cg_s ) & ( grid_df['part'] > 3 ) ]\n",
    "                        test_y = y[ ( grid_df['sess'] == cg_s ) & ( grid_df['part'] > 3 ) ]      \n",
    "\n",
    "                        # predict\n",
    "                        acc[ si, r_idx, s-1, cg_s-1, p-1 ] = model.score( test_X,test_y )\n",
    "                    \n",
    "                else:\n",
    "                \n",
    "                    # training data from this session, all parts except current part...\n",
    "                    train_X = X[ ( grid_df['sess'] == s ) & ( grid_df['part'] > 3 ) ]\n",
    "                    train_y = y[ ( grid_df['sess'] == s ) & ( grid_df['part'] > 3 ) ]\n",
    "\n",
    "                    # train model...\n",
    "                    model.fit( train_X, train_y )\n",
    "\n",
    "                    # test data same session\n",
    "                    test_X = X[ ( grid_df['sess'] == s ) & ( grid_df['part'] < 4 ) ]\n",
    "                    test_y = y[ ( grid_df['sess'] == s ) & ( grid_df['part'] < 4 ) ]\n",
    "\n",
    "                    # predict\n",
    "                    acc[ si, r_idx, s-1, s-1, p-1 ] = model.score( test_X,test_y )\n",
    "\n",
    "                    # cross-gen from current session to other sessions...\n",
    "                    for cg_s in np.setdiff1d( range( 1,n_sess+1 ), s ):\n",
    "\n",
    "                        # test data same session\n",
    "                        test_X = X[ ( grid_df['sess'] == cg_s ) & ( grid_df['part'] < 4 ) ]\n",
    "                        test_y = y[ ( grid_df['sess'] == cg_s ) & ( grid_df['part'] < 4 ) ]      \n",
    "\n",
    "                        # predict\n",
    "                        acc[ si, r_idx, s-1, cg_s-1, p-1 ] = model.score( test_X,test_y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc[:,0,:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array([grid_df['sess'],grid_df['part'],grid_df['run_in_part'],grid_df['task']]).T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r_idx,r in enumerate( roi_names ):\n",
    "  \n",
    "    md = np.nanmean(acc[:,r_idx,:,:,:2],axis=0)\n",
    "    md = np.nanmean(md,axis=2)\n",
    "\n",
    "    plt.imshow(md, cmap = 'RdBu',vmin = .33, vmax = .4)\n",
    "    \n",
    "    plt.title(r)\n",
    "    \n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First filter the `is_main_grid` trials out and grab a few constants we'll use for parsing the data during decoding..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new df that only has main grid trials\n",
    "grid_df = main_labels[ main_labels[ 'is_main_grid' ] == 1 ]\n",
    "\n",
    "# filter the data from each roi to only include main grid trials\n",
    "# move from list to dict so that roi name is the key (just my preference)\n",
    "grid_data = {}\n",
    "\n",
    "for r_idx, r in enumerate( roi_names ):\n",
    "    \n",
    "    grid_data[ r ] = main_data[ r_idx ][ main_labels[ 'is_main_grid' ] == 1 ] \n",
    "    \n",
    "    \n",
    "# how many sessions\n",
    "n_sess = np.max( np.max( grid_df[ 'sess' ] ) )\n",
    "\n",
    "# how many parts in each session \n",
    "n_parts = np.max( grid_df[ 'part' ] ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now assign label to each of the 16 positions...this is clunky, but whatever..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all x,y values\n",
    "x = np.array( grid_df[ 'ptx' ] )\n",
    "y = np.array( grid_df[ 'pty' ] )\n",
    "\n",
    "# meshgrid for all combos\n",
    "X,Y = np.meshgrid( np.unique(x),np.unique(y) )\n",
    "\n",
    "# figure out all unique x,y combos and put into array\n",
    "pnts = np.vstack( ( X.ravel(),Y.ravel() ) ).T\n",
    "\n",
    "# to store grid pos of each stim\n",
    "grid_pos = np.full( len( x ),np.nan )\n",
    "\n",
    "# loop over all trials\n",
    "for i in range( len( x ) ):\n",
    "    \n",
    "    # loop over unique points\n",
    "    for j in range( len( pnts ) ):\n",
    "        \n",
    "        # find the match for current x,y pair\n",
    "        if np.sum( [x[i],y[i]]==pnts[j,] ) == 2:\n",
    "            \n",
    "            # store\n",
    "            grid_pos[i] = j\n",
    "            \n",
    "# double check num unique vals + overall length\n",
    "assert( len( np.unique( grid_pos ) ) == 16 )\n",
    "assert( len( grid_pos ) == grid_data['V1'].shape[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do decoding using multinomial logistic regression - here doing within and across-session generalization using LOO (leave one part out in this case)\n",
    "* note: `roi_names == ['V1', 'V2', 'V3', 'V3AB', 'hV4', 'IPS0', 'IPS1', 'IPS2', 'IPS3', 'LO1', 'LO2']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# penalties to eval\n",
    "num_cs = 10\n",
    "Cs = np.logspace( -5,0,num_cs )\n",
    "\n",
    "# define model\n",
    "# model = LogisticRegressionCV( Cs=Cs, cv = n_parts-1, multi_class='multinomial', solver='lbfgs', penalty='l2' )\n",
    "model = LogisticRegressionCV( cv = n_parts-1, multi_class='multinomial', solver='lbfgs', penalty='l2', n_jobs=-1 )\n",
    "\n",
    "# store acc...\n",
    "acc = np.full( ( len( roi_names ), n_sess, n_sess, n_parts ), np.nan )\n",
    "\n",
    "# loop over ROIs\n",
    "for r_idx, r in enumerate( roi_names[:2] ):\n",
    "\n",
    "    # get the data from this ROI\n",
    "    X = grid_data[ r ]\n",
    "\n",
    "    # loop over sessions...train/test on all combos (e.g. sess1-1, sess1-2, sess1-3, etc)    \n",
    "    for s in np.arange( 1,n_sess+1 ):\n",
    "\n",
    "        # progress...\n",
    "        print(f'ROI: {r}, Session: {s}')\n",
    "        \n",
    "        # loop over parts\n",
    "        for p in np.arange( 1,n_parts+1 ):\n",
    "              \n",
    "            # training data from this session, all parts except current part...\n",
    "            train_X = X[ ( grid_df['sess'] == s ) & ( grid_df['part'] != p ) ]\n",
    "            train_y = grid_pos[ ( grid_df['sess'] == s ) & ( grid_df['part'] != p ) ]\n",
    "\n",
    "            # train model...\n",
    "            model.fit( train_X, train_y )\n",
    "            \n",
    "            # test data same session\n",
    "            test_X = X[ ( grid_df['sess'] == s ) & ( grid_df['part'] == p ) ]\n",
    "            test_y = grid_pos[ ( grid_df['sess'] == s ) & ( grid_df['part'] == p ) ]\n",
    "\n",
    "            # predict\n",
    "            acc[ r_idx, s-1, s-1, p-1 ] = model.score( test_X,test_y )\n",
    "            \n",
    "            # cross-gen from current session to other sessions...\n",
    "            for cg_s in np.setdiff1d( range( 1,n_sess+1 ), s ):\n",
    "\n",
    "                # test data same session\n",
    "                test_X = X[ ( grid_df['sess'] == cg_s ) & ( grid_df['part'] == p ) ]\n",
    "                test_y = grid_pos[ ( grid_df['sess'] == cg_s ) & ( grid_df['part'] == p ) ]      \n",
    "                \n",
    "                # predict\n",
    "                acc[ r_idx, s-1, cg_s-1, p-1 ] = model.score( test_X,test_y )\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na = np.mean(acc[0,:,:,:], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow( na )\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples= 1000, n_features= 9,\n",
    "                           n_classes=2,random_state=42)\n",
    "\n",
    "train_X = X[:900,:]\n",
    "train_y = y[:900]\n",
    "test_X = X[900:,:]\n",
    "test_y = y[900:]\n",
    "\n",
    "model = LogisticRegressionCV( cv = 10, multi_class='multinomial', solver='lbfgs', penalty='l2' )\n",
    "\n",
    "model.fit(train_X,train_y)\n",
    "cv_acc = model.score(test_X,test_y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick plot of classification acc as a function of penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot( pens, acc, linewidth = 2 )\n",
    "plt.hlines( 1/16, pens[0], pens[-1] )\n",
    "plt.xscale( 'log' )\n",
    "plt.ylim( [ 0, 0.5 ] )\n",
    "plt.xlabel( 'Penalty' )\n",
    "plt.ylabel( 'Accuracy' )\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
